{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5af9a535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Dependecies\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors,Word2Vec\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5b953c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Synonym File\n",
    "synonym_file = pd.read_csv(\"synonym.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a7d30ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize the Model Name\n",
    "model_name_1 = \"word2vec-google-news-300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20537b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data model\n",
    "data_model_1 = api.load(model_name_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f4fcdfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def detail_helper(data_model,file_name) :\n",
    "    correct_answer = 0\n",
    "    guess = 0\n",
    "    Found = False\n",
    "    with open(f'{file_name}.csv','w') as file:\n",
    "        file.write(\"question,answer,guess,label\\n\")\n",
    "        for iter,row in synonym_file.iterrows():\n",
    "            question_word = row['question']\n",
    "            answer_word = row['answer']\n",
    "          \n",
    "            if (question_word in data_model and any(row[f'{i}'] in data_model for i in range(0,4))):\n",
    "                similarities = [(row[f'{i}'], data_model.similarity(question_word, row[f'{i}'])) for i in range(0,4) if row[f'{i}'] in data_model]\n",
    "                closest,_ = max(similarities, key=lambda x: x[1])\n",
    "                if (closest == answer_word):\n",
    "                    file.write(f'{question_word},{answer_word},{closest},correct\\n')\n",
    "                    correct_answer+= 1\n",
    "                    if(Found == False):\n",
    "                        guess += 1\n",
    "                else:\n",
    "                    file.write(f'{question_word},{answer_word},{closest},wrong\\n')\n",
    "                    if(Found == False):\n",
    "                        guess += 1      \n",
    "            else:\n",
    "                random_word = row[f'{random.randint(0,3)}']\n",
    "                file.write(f'{question_word},{answer_word},{random_word},guess\\n')\n",
    "                Found = True\n",
    "    \n",
    "    with open('analysis.csv','a') as analysis:\n",
    "        analysis.write(f'{file_name},{len(data_model)},{correct_answer},{guess},{correct_answer/guess}\\n')\n",
    "    \n",
    "    return {'file_name' : file_name , 'length' : len(data_model),'correct_answer' : correct_answer, 'guess' : guess, 'accuracy' : correct_answer/guess }\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ea21378a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'hello',\n",
       " 'length': 3000000,\n",
       " 'correct_answer': 70,\n",
       " 'guess': 63,\n",
       " 'accuracy': 1.1111111111111112}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call function to create analysis files\n",
    "detail_helper(data_model_1,word2vec-google-news-300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9d4ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "# First Model -- Same Embeddings\n",
    "model_name_2 = \"glove-twitter-50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "320589b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "data_model_2 = api.load(model_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ff1c5b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'glove-twitter-50',\n",
       " 'length': 1193514,\n",
       " 'correct_answer': 36,\n",
       " 'guess': 36,\n",
       " 'accuracy': 1.0}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the Helper Function \n",
    "detail_helper(data_model_2,model_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84bc0cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Model -- Same Emeddings\n",
    "model_name_3 = \"glove-wiki-gigaword-50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f59640b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "data_model_3 = api.load(model_name_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "40d2e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Helper\n",
    "detail_helper(data_model_3,model_name_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b15553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Model -- Different Embedding same Corpus\n",
    "model_name_4 = \"glove-wiki-gigaword-200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74c11e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 252.1/252.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "data_model_4 = api.load(model_name_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31ad18f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Helper\n",
    "detail_helper(data_model_4,model_name_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c26b5642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth Model -- Different Embedding same corpus\n",
    "model_name_5 = \"glove-wiki-gigaword-300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b683c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data_model_5 = api.load(model_name_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7b3341e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': 'glove-wiki-gigaword-300', 'length': 400000, 'correct_answer': 71, 'guess': 80, 'accuracy': 0.8875}\n"
     ]
    }
   ],
   "source": [
    "# Call the Helper\n",
    "print(detail_helper(data_model_5,model_name_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e788b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Analysis Random Baseline -- Human Gold Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b714a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "# Array of Books\n",
    "book_array = ['./pg72232.txt','./pg72233.txt','./pg72234.txt','./pg72235.txt','./pg72236.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "93298a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_books(book_array):\n",
    "    sentences = []\n",
    "    for i in book_array:\n",
    "        with open(i, 'r',encoding='utf-8') as file:\n",
    "            sentences.append(sent_tokenize(file.read()))\n",
    "    \n",
    "    return sentences\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "08d84b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the constants\n",
    "# Window Size Used\n",
    "window_size = [5,10]\n",
    "# Number of Embeddings\n",
    "embeddings = [100,200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6b23b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with sentences\n",
    "def train_model(sentence_array,window_size,embeddinsg) :\n",
    "    model = Word2Vec(sentence_array,vector_size=embeddings,window=window_size)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "02c7958f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_model_1 \u001b[38;5;241m=\u001b[39m train_model(process_books(book_array),window_size[\u001b[38;5;241m0\u001b[39m],embeddings[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      2\u001b[0m detail_helper(train_model_1,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mown-corpus-E\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-W\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_size[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[107], line 3\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(sentence_array, window_size, embeddinsg)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(sentence_array,window_size,embeddinsg) :\n\u001b[1;32m----> 3\u001b[0m     model \u001b[38;5;241m=\u001b[39m Word2Vec(sentence_array,vector_size\u001b[38;5;241m=\u001b[39membeddings,window\u001b[38;5;241m=\u001b[39mwindow_size)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\word2vec.py:378\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03mOnce you're finished training a model (=no more updates, only querying)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m corpus_iterable \u001b[38;5;241m=\u001b[39m sentences\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(vector_size)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(workers)\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m epochs\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
     ]
    }
   ],
   "source": [
    "train_model_1 = train_model(process_books(book_array),window_size[0],embeddings[0])\n",
    "detail_helper(train_model_1,f'own-corpus-E{embeddings[0]}-W{window_size[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
